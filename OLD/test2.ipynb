{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_1\n",
      "df_2\n",
      "df_3\n",
      "df_4\n",
      "df_5\n",
      "df_6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#_______________________________________________________________\n",
    "# Function to fix some bugs\n",
    "def process_dataframe(df):\n",
    "    # Check if values from first column to column 1289 in the last row are NaN\n",
    "    if df.iloc[-1, :1289].isna().all():\n",
    "        # Assign the values contained in columns 1290 and 1291 of the last row to variables\n",
    "        col_1290_value = df.iloc[-1, 1289]\n",
    "        col_1291_value = df.iloc[-1, 1290]\n",
    "        \n",
    "        # Delete the last row\n",
    "        df = df.iloc[:-1, :]\n",
    "        \n",
    "        # Assign the values to the new last row\n",
    "        df.iloc[-1, 1289] = col_1290_value\n",
    "        df.iloc[-1, 1290] = col_1291_value\n",
    "    \n",
    "    return df\n",
    "#__________________________________________________________________\n",
    "# Directory containing the txt files\n",
    "directory = r'C:\\Users\\e2023898\\Downloads\\test'\n",
    "\n",
    "# List to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for i, filename in enumerate(os.listdir(directory)):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            # First method to read the file\n",
    "            df = pd.read_csv(file_path, delimiter=\"\\t\", header=None)\n",
    "            df = df.drop(17).reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            # Second method to read the file if the first fails\n",
    "            total_columns = 321\n",
    "            df1 = pd.read_csv(file_path, header=None, engine='python', encoding='latin', sep='\\t', nrows=17)\n",
    "            df2 = pd.read_csv(file_path, header=None, engine='python', encoding='latin', sep='\\t', skiprows=18)\n",
    "            df = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "        \n",
    "        # Add 'File name' and 'Last modification' columns at row 20\n",
    "        df.loc[20, 'File name'] = filename\n",
    "        df.loc[20, 'Last modification'] = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dataframes.append(df)\n",
    "        \n",
    "        # Assign the dataframe to a variable name sequentially\n",
    "        globals()[f'df_{i+1}'] = df\n",
    "\n",
    "        # Print the names of the dataframes\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f'df_{i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:53: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:53: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:53: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:53: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:53: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:53: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:53: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:53: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "c:\\Users\\e2023898\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "# Function to append columns to riassunto dataframes\n",
    "def append_columns_to_riassunto(riassunto, columns, values):\n",
    "    for col, val in zip(columns, values):\n",
    "        riassunto[col] = val\n",
    "    return riassunto\n",
    "\n",
    "riassunto_dataframes = []\n",
    "\n",
    "\n",
    "# Process each dataframe and create riassunto dataframes\n",
    "for i, df in enumerate(dataframes):\n",
    "    # Split the dataframe\n",
    "    test_id = df.iloc[:17]\n",
    "    test_id = test_id.loc[:, test_id.notna().any(axis=0)]\n",
    "    index_row = df.iloc[17]\n",
    "    remaining_df = df.iloc[17:]\n",
    "    test_data = test_id.transpose()\n",
    "    test_data.columns = test_data.iloc[0]\n",
    "    test_data = test_data[1:]\n",
    "    test_data_columns = test_data.columns.tolist()\n",
    "    test_data_values = test_data.iloc[0].tolist()\n",
    "\n",
    "    # Initialize variables\n",
    "    datasets = {}\n",
    "    current_dataset = []\n",
    "    dataset_counter = 1\n",
    "    note_counter = 0\n",
    "\n",
    "    # Process the remaining rows\n",
    "    for index, row in remaining_df.iterrows():\n",
    "        if row[0] == \"Note\":\n",
    "            note_counter += 1\n",
    "            if note_counter > 1:\n",
    "                datasets[f\"dataset_{dataset_counter}\"] = pd.DataFrame(current_dataset)\n",
    "                dataset_counter += 1\n",
    "                current_dataset = []\n",
    "        current_dataset.append(row)\n",
    "\n",
    "    # Append the last dataset if exists\n",
    "    if current_dataset:\n",
    "        datasets[f\"dataset_{dataset_counter}\"] = pd.DataFrame(current_dataset)\n",
    "\n",
    "    # Create a dataframe for each dataset and show the names of the dataframes created\n",
    "    dataframe_names = []\n",
    "    for key, dataset in datasets.items():\n",
    "        globals()[key] = dataset\n",
    "        dataframe_names.append(key)\n",
    "\n",
    "    # Apply the operations to each dataset\n",
    "    for key, dataset in datasets.items():\n",
    "        # Convert the second column to datetime, coercing errors\n",
    "        dataset.iloc[3:, 1] = pd.to_datetime(dataset.iloc[3:, 1], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "        \n",
    "        # Replace all commas with periods in columns 2 to the end\n",
    "        dataset.iloc[:, 1:] = dataset.iloc[:, 1:].replace(',', '.', regex=True)\n",
    "        \n",
    "        # Convert subsequent columns to float, coercing errors\n",
    "        for col in dataset.columns[2:]:\n",
    "            dataset.iloc[2:, dataset.columns.get_loc(col)] = pd.to_numeric(dataset.iloc[2:, dataset.columns.get_loc(col)], errors='coerce')\n",
    "        \n",
    "        # Update the dataset in the dictionary\n",
    "        datasets[key] = dataset\n",
    "\n",
    "    # Create the dataframe for storing general data\n",
    "    riassunto = pd.DataFrame(columns=index_row)\n",
    "    new_columns = []\n",
    "    riassunto = pd.concat([riassunto, pd.DataFrame(columns=new_columns)], axis=1)\n",
    "\n",
    " # Calculate and assign time delta values for the second column of riassunto\n",
    "    for key, dataset in datasets.items():\n",
    "        # Ensure the values are Timestamps\n",
    "        start_time = pd.to_datetime(dataset.iloc[3, 1], errors='coerce')\n",
    "        end_time = pd.to_datetime(dataset.iloc[-1, 1], errors='coerce')\n",
    "        time_delta = end_time - start_time\n",
    "        row_index = dataframe_names.index(key)\n",
    "        riassunto.at[row_index, riassunto.columns[1]] = time_delta\n",
    "\n",
    "    # Assign values from datasets to riassunto sequentially in column 1\n",
    "    row_counter = 0\n",
    "    for key, dataset in datasets.items():\n",
    "        value_to_assign = dataset.iloc[1, 0]  # Value from column 1 row 2\n",
    "        riassunto.at[row_counter, 'Note'] = value_to_assign\n",
    "        row_counter += 1\n",
    "\n",
    "    # Iterate over each dataset\n",
    "    for key, dataset in datasets.items():\n",
    "        # Calculate the average values for each column starting from the third column\n",
    "        averages = dataset.iloc[2:, 2:].mean()\n",
    "        \n",
    "        # Assign the average values to the corresponding row in riassunto\n",
    "        row_index = dataframe_names.index(key)\n",
    "        for col_index, avg_value in enumerate(averages, start=2):\n",
    "            riassunto.iloc[row_index, col_index] = avg_value\n",
    "\n",
    "    # Create new columns for minimum and maximum values\n",
    "    avg_columns = []\n",
    "    min_columns = []\n",
    "    max_columns = []\n",
    "\n",
    "    for col in riassunto.columns[1:]:\n",
    "        avg_columns.append(f\"{col}_std\")\n",
    "        min_columns.append(f\"{col}_min\")\n",
    "        max_columns.append(f\"{col}_max\")\n",
    "        \n",
    "\n",
    "    # Append new columns to riassunto dataframe\n",
    "    riassunto = pd.concat([riassunto, pd.DataFrame(columns=avg_columns + min_columns + max_columns)], axis=1)\n",
    "\n",
    "    # Iterate over each dataset\n",
    "    for key, dataset in datasets.items():\n",
    "        # Calculate the minimum and maximum values for each column starting from the third column\n",
    "        deviations = dataset.iloc[2:, 2:].std()\n",
    "        min_values = dataset.iloc[2:, 2:].min()\n",
    "        max_values = dataset.iloc[2:, 2:].max()\n",
    "        \n",
    "        # Assign the minimum values to the corresponding row in riassunto\n",
    "        row_index = dataframe_names.index(key)\n",
    "        for col_index, deviations in enumerate(deviations, start=2):\n",
    "            riassunto.iloc[row_index, len(riassunto.columns)//3 + col_index] = deviations\n",
    "        \n",
    "        for col_index, min_value in enumerate(min_values, start=2):\n",
    "            riassunto.iloc[row_index, len(riassunto.columns)//3 + col_index] = min_value\n",
    "        \n",
    "        # Assign the maximum values to the corresponding row in riassunto\n",
    "        for col_index, max_value in enumerate(max_values, start=2):\n",
    "            riassunto.iloc[row_index, 2*len(riassunto.columns)//3 + col_index] = max_value\n",
    "\n",
    "\n",
    "    # Assign 'File name' and 'Last modification' values from row 20 of each dataframe\n",
    "    riassunto['File name'] = df.at[20,'File name']\n",
    "    riassunto['Last modification'] = df.at[20,'Last modification']\n",
    "    #riassunto.at[20, 'File name'] = df.at[20, 'File name']\n",
    "    #riassunto.at[20, 'Last modification'] = df.at[20, 'Last modification']\n",
    "    updated_riassunto = append_columns_to_riassunto(riassunto, test_data_columns, test_data_values) \n",
    "\n",
    "\n",
    "    # Assign the riassunto dataframe to a variable name sequentially\n",
    "    globals()[f'riassunto_{i+1}'] = updated_riassunto\n",
    "    riassunto_dataframes.append(updated_riassunto)\n",
    "\n",
    "for i in range(1, len(globals())):\n",
    "    var_name = f'riassunto_{i}'\n",
    "    if var_name in globals():\n",
    "        globals()[var_name] = process_dataframe(globals()[var_name])\n",
    "\n",
    "# Print the processed riassunto dataframes for verification\n",
    "for i in range(1, len(globals())):\n",
    "    var_name = f'riassunto_{i}'\n",
    "\n",
    "header = riassunto_1.columns.tolist()\n",
    "# Loop through each DataFrame and replace column names with values from 'header'\n",
    "for i, df in enumerate(riassunto_dataframes):\n",
    "    df.columns = header\n",
    "\n",
    "for i in range(len(riassunto_dataframes)):\n",
    "    riassunto_dataframes[i]=riassunto_dataframes[i].loc[~riassunto_dataframes[i].index.to_series().duplicated(keep='first')]\n",
    "\n",
    "final_dataframe = pd.concat(riassunto_dataframes, ignore_index=True)\n",
    "\n",
    "# Sort the dataframe by 'DATA/ORA COLLAUDO'\n",
    "final_dataframe.sort_values(by='Last modification', inplace=True)\n",
    "\n",
    "# Create a new column 'numero.test' with sequential values based on the sorted dates\n",
    "final_dataframe['numero.test'] = final_dataframe.groupby('File name').ngroup().apply(lambda x: f'P{x+1:06d}')\n",
    "\n",
    "# Ensure that rows with the same 'File Name' have the same 'numero.test'\n",
    "final_dataframe['numero.test'] = final_dataframe.groupby('File name')['numero.test'].transform('first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_1\n",
      "df_2\n",
      "df_3\n",
      "df_4\n",
      "df_5\n",
      "df_6\n",
      "df_7\n",
      "df_8\n",
      "df_9\n",
      "df_10\n",
      "df_11\n",
      "df_12\n",
      "df_13\n",
      "df_14\n",
      "df_15\n",
      "df_16\n",
      "df_17\n",
      "df_18\n",
      "df_19\n",
      "df_20\n",
      "df_21\n",
      "df_22\n",
      "df_23\n",
      "df_24\n",
      "df_25\n",
      "df_26\n",
      "df_27\n",
      "df_28\n",
      "df_29\n",
      "df_30\n",
      "df_31\n",
      "df_32\n",
      "df_33\n",
      "df_34\n",
      "df_35\n",
      "df_36\n",
      "df_37\n",
      "df_38\n",
      "df_39\n",
      "df_40\n",
      "df_41\n",
      "df_42\n",
      "df_43\n",
      "df_44\n",
      "df_45\n",
      "df_46\n",
      "df_47\n",
      "df_48\n",
      "df_49\n",
      "df_50\n",
      "df_51\n",
      "df_52\n",
      "df_53\n",
      "df_54\n",
      "df_55\n",
      "df_56\n",
      "df_57\n",
      "df_58\n",
      "df_59\n",
      "df_60\n",
      "df_61\n",
      "df_62\n",
      "df_63\n",
      "df_64\n",
      "df_65\n",
      "df_66\n",
      "df_67\n",
      "df_68\n",
      "df_69\n",
      "df_70\n",
      "df_71\n",
      "df_72\n",
      "df_73\n",
      "df_74\n",
      "df_75\n",
      "df_76\n",
      "df_77\n",
      "df_78\n",
      "df_79\n",
      "df_80\n",
      "df_81\n",
      "df_82\n",
      "df_83\n",
      "df_84\n",
      "df_85\n",
      "df_86\n",
      "df_87\n",
      "df_88\n",
      "df_89\n",
      "df_90\n",
      "df_91\n",
      "df_92\n",
      "df_93\n",
      "df_94\n",
      "df_95\n",
      "df_96\n",
      "df_97\n",
      "df_98\n",
      "df_99\n",
      "df_100\n",
      "df_101\n",
      "df_102\n",
      "df_103\n",
      "df_104\n",
      "df_105\n",
      "df_106\n",
      "df_107\n",
      "df_108\n",
      "df_109\n",
      "df_110\n",
      "df_111\n",
      "df_112\n",
      "df_113\n",
      "df_114\n",
      "df_115\n",
      "df_116\n",
      "df_117\n",
      "df_118\n",
      "df_119\n",
      "df_120\n",
      "df_121\n",
      "df_122\n",
      "df_123\n",
      "df_124\n",
      "df_125\n",
      "df_126\n",
      "df_127\n",
      "df_128\n",
      "df_129\n",
      "df_130\n",
      "df_131\n",
      "df_132\n",
      "df_133\n",
      "df_134\n",
      "df_135\n",
      "df_136\n",
      "df_137\n",
      "df_138\n",
      "df_139\n",
      "df_140\n",
      "df_141\n",
      "df_142\n",
      "df_143\n",
      "df_144\n",
      "df_145\n",
      "df_146\n",
      "df_147\n",
      "df_148\n",
      "df_149\n",
      "df_150\n",
      "df_151\n",
      "df_152\n",
      "df_153\n",
      "df_154\n",
      "df_155\n",
      "df_156\n",
      "df_157\n",
      "df_158\n",
      "df_159\n",
      "df_160\n",
      "df_161\n",
      "df_162\n",
      "df_163\n",
      "df_164\n",
      "df_165\n",
      "df_166\n",
      "df_167\n",
      "df_168\n",
      "df_169\n",
      "df_170\n",
      "df_171\n",
      "df_172\n",
      "df_173\n",
      "df_174\n",
      "df_175\n",
      "df_176\n",
      "df_177\n",
      "df_178\n",
      "df_179\n",
      "df_180\n",
      "df_181\n",
      "df_182\n",
      "df_183\n",
      "df_184\n",
      "df_185\n",
      "df_186\n",
      "df_187\n",
      "df_188\n",
      "df_189\n",
      "df_190\n",
      "df_191\n",
      "df_192\n",
      "df_193\n",
      "df_194\n",
      "df_195\n",
      "df_196\n",
      "df_197\n",
      "df_198\n",
      "df_199\n",
      "df_200\n",
      "df_201\n",
      "df_202\n",
      "df_203\n",
      "df_204\n",
      "df_205\n",
      "df_206\n",
      "df_207\n",
      "df_208\n",
      "df_209\n",
      "df_210\n",
      "df_211\n",
      "df_212\n",
      "df_213\n",
      "df_214\n",
      "df_215\n",
      "df_216\n",
      "df_217\n",
      "df_218\n",
      "df_219\n",
      "df_220\n",
      "df_221\n",
      "df_222\n",
      "df_223\n",
      "df_224\n",
      "df_225\n",
      "df_226\n",
      "df_227\n",
      "df_228\n",
      "df_229\n",
      "df_230\n",
      "df_231\n",
      "df_232\n",
      "df_233\n",
      "df_234\n",
      "df_235\n",
      "df_236\n",
      "df_237\n",
      "df_238\n",
      "df_239\n",
      "df_240\n",
      "df_241\n",
      "df_242\n",
      "df_243\n",
      "df_244\n",
      "df_245\n",
      "df_246\n",
      "df_247\n",
      "df_248\n",
      "df_249\n",
      "df_250\n",
      "df_251\n",
      "df_252\n",
      "df_253\n",
      "df_254\n",
      "df_255\n",
      "df_256\n",
      "df_257\n",
      "df_258\n",
      "df_259\n",
      "df_260\n",
      "df_261\n",
      "df_262\n",
      "df_263\n",
      "df_264\n",
      "df_265\n",
      "df_266\n",
      "df_267\n",
      "df_268\n",
      "df_269\n",
      "df_270\n",
      "df_271\n",
      "df_272\n",
      "df_273\n",
      "df_274\n",
      "df_275\n",
      "df_276\n",
      "df_277\n",
      "df_278\n",
      "df_279\n",
      "df_280\n",
      "df_281\n",
      "df_282\n",
      "df_283\n",
      "df_284\n",
      "df_285\n",
      "df_286\n",
      "df_287\n",
      "df_288\n",
      "df_289\n",
      "df_290\n",
      "df_291\n",
      "df_292\n",
      "df_293\n",
      "df_294\n",
      "df_295\n",
      "df_296\n",
      "df_297\n",
      "df_298\n",
      "df_299\n",
      "df_300\n",
      "df_301\n",
      "df_302\n",
      "df_303\n",
      "df_304\n",
      "df_305\n",
      "df_306\n",
      "df_307\n",
      "df_308\n",
      "df_309\n",
      "df_310\n",
      "df_311\n",
      "df_312\n",
      "df_313\n",
      "df_314\n",
      "df_315\n",
      "df_316\n",
      "df_317\n",
      "df_318\n",
      "df_319\n",
      "df_320\n",
      "df_321\n",
      "df_322\n",
      "df_323\n",
      "df_324\n",
      "df_325\n",
      "df_326\n",
      "df_327\n",
      "df_328\n",
      "df_329\n",
      "df_330\n",
      "df_331\n",
      "df_332\n",
      "df_333\n",
      "df_334\n",
      "df_335\n",
      "df_336\n",
      "df_337\n",
      "df_338\n",
      "df_339\n",
      "df_340\n",
      "df_341\n",
      "df_342\n",
      "df_343\n",
      "df_344\n",
      "df_345\n",
      "df_346\n",
      "df_347\n",
      "df_348\n",
      "df_349\n",
      "df_350\n",
      "df_351\n",
      "df_352\n",
      "df_353\n",
      "df_354\n",
      "df_355\n",
      "df_356\n",
      "df_357\n",
      "df_358\n",
      "df_359\n",
      "df_360\n",
      "df_361\n",
      "df_362\n",
      "df_363\n",
      "df_364\n",
      "df_365\n",
      "df_366\n",
      "df_367\n",
      "df_368\n",
      "df_369\n",
      "df_370\n",
      "df_371\n",
      "df_372\n",
      "df_373\n",
      "df_374\n",
      "df_375\n",
      "df_376\n",
      "df_377\n",
      "df_378\n",
      "df_379\n",
      "df_380\n",
      "df_381\n",
      "df_382\n",
      "df_383\n",
      "df_384\n",
      "df_385\n",
      "df_386\n",
      "df_387\n",
      "df_388\n",
      "df_389\n",
      "df_390\n",
      "df_391\n",
      "df_392\n",
      "df_393\n",
      "df_394\n",
      "df_395\n",
      "df_396\n",
      "df_397\n",
      "df_398\n",
      "df_399\n",
      "df_400\n",
      "df_401\n",
      "df_402\n",
      "df_403\n",
      "df_404\n",
      "df_405\n",
      "df_406\n",
      "df_407\n",
      "df_408\n",
      "df_409\n",
      "df_410\n",
      "df_411\n",
      "df_412\n",
      "df_413\n",
      "df_414\n",
      "df_415\n",
      "Skipped files: ['00312386_TALA8NMSBA10000R03_20230206_105757.txt', '00312386_TALA8NMSBA10000R03_20230206_115156.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230213_070657.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230213_113726.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230214_085849.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230215_070409.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230216_070625.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230216_110150.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230220_123518.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230224_105251.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230224_113632.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230227_080206.txt', '00312855_PROTOTIPO_TALA3_GROBR00_20230228_080215.txt', '00314070_TAO93NMIBXGT100R01_20230223_145203.txt', '00314070_TAO93NMIBXGT100R01_20230224_070230 - Copia.txt', '00314070_TAO93NMIBXGT100R01_20230224_070230.txt', '00314070_TAO93NMIBXGT100R01_20230225_070746.txt', '00314804_TAL37NBSBXED200R05_20230213_140859.txt', '00314804_TAL37NBSBXED200R05_20230214_072859.txt', '00314804_TAL37NBSBXED200R05_20230214_072859_1.txt', '00314854_TAL93NNHBA10098R00_20230228_140527.txt', '00314854_TAL93NNHBA10098R00_20230301_080034.txt', '00314854_TAL93NNHBA10098R00_20230302_112926.txt', '00314854_TAL93NNHBA10098R00_20230303_082505.txt', '00314856_TAOA3PLVBXIS000R04_20230208_151013.txt', '00314856_TAOA3PLVBXIS000R04_20230209_070652.txt', '00315015_TAL24NBSBA00000R04_20230217_114405 - Copia (2).txt', '00315015_TAL24NBSBA00000R04_20230217_114405 - Copia.txt', '00315015_TAL24NBSBA00000R04_20230217_114405.txt', '00315015_TAL24NBSBA00000R04_20230220_123919 - Copia.txt', '00315015_TAL24NBSBA00000R04_20230220_123919.txt', '00315015_TAL24NBSBA00000R04_20230222_101538.txt', '00315032_TCW19NBSBXED000R03_20230215_152137.txt', '00315032_TCW19NBSBXED000R03_20230216_085343.txt', '00315032_TCW19NBSBXED000R03_20230216_112958.txt', '00315158_TAL37NHHBXTC000R00_20230203_092503.txt', '00315158_TAL37NHHBXTC000R00_20230204_073843.txt', '00315158_TAL37NHHBXTC000R00_20230207_130806.txt', '00315158_TAL37NHHBXTC000R00_20230208_070202.txt', '00315692_TALB9NHHBXET000R00_20230207_084905.txt', '00316312_TALE6NMSBXUP800R03_20230228_080357.txt', '00317467_CCWA9NHHBXOS011R00_20230929_103102_1.txt', '00317778_TCW19NBSBA10023R01_20230204_071007.txt', '00324111_SAW50RBSBXSC100R05_20230302_090333.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to fix some bugs\n",
    "def process_dataframe(df):\n",
    "    # Check if values from first column to column 1289 in the last row are NaN\n",
    "    if df.iloc[-1, :1289].isna().all():\n",
    "        # Assign the values contained in columns 1290 and 1291 of the last row to variables\n",
    "        col_1290_value = df.iloc[-1, 1289]\n",
    "        col_1291_value = df.iloc[-1, 1290]\n",
    "        \n",
    "        # Delete the last row\n",
    "        df = df.iloc[:-1, :]\n",
    "        \n",
    "        # Assign the values to the new last row\n",
    "        df.iloc[-1, 1289] = col_1290_value\n",
    "        df.iloc[-1, 1290] = col_1291_value\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Directory containing the txt files\n",
    "directory = r'V:\\TEST CHILLER\\TXT'\n",
    "\n",
    "# List to store dataframes and skipped files\n",
    "dataframes = []\n",
    "skipped_files = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for i, filename in enumerate(os.listdir(directory)):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            # First method to read the file\n",
    "            df = pd.read_csv(file_path, delimiter=\"\\t\", header=None)\n",
    "            df = df.drop(17).reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # Second method to read the file if the first fails\n",
    "                total_columns = 321\n",
    "                df1 = pd.read_csv(file_path, header=None, engine='python', encoding='latin', sep='\\t', nrows=17)\n",
    "                df2 = pd.read_csv(file_path, header=None, engine='python', encoding='latin', sep='\\t', skiprows=18)\n",
    "                df = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                # If both methods fail, skip the file and add to skipped_files list\n",
    "                skipped_files.append(filename)\n",
    "                continue\n",
    "        \n",
    "        # Add 'File name' and 'Last modification' columns at row 20\n",
    "        df.loc[20, 'File name'] = filename\n",
    "        df.loc[20, 'Last modification'] = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dataframes.append(df)\n",
    "        \n",
    "        # Assign the dataframe to a variable name sequentially\n",
    "        globals()[f'df_{i+1}'] = df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append columns to riassunto dataframes\n",
    "def append_columns_to_riassunto(riassunto, columns, values):\n",
    "    for col, val in zip(columns, values):\n",
    "        riassunto[col] = val\n",
    "    return riassunto\n",
    "\n",
    "riassunto_dataframes = []\n",
    "\n",
    "\n",
    "# Process each dataframe and create riassunto dataframes\n",
    "for i, df in enumerate(dataframes):\n",
    "    # Split the dataframe\n",
    "    test_id = df.iloc[:17]\n",
    "    test_id = test_id.loc[:, test_id.notna().any(axis=0)]\n",
    "    index_row = df.iloc[17]\n",
    "    remaining_df = df.iloc[17:]\n",
    "    test_data = test_id.transpose()\n",
    "    test_data.columns = test_data.iloc[0]\n",
    "    test_data = test_data[1:]\n",
    "    test_data_columns = test_data.columns.tolist()\n",
    "    test_data_values = test_data.iloc[0].tolist()\n",
    "\n",
    "    # Initialize variables\n",
    "    datasets = {}\n",
    "    current_dataset = []\n",
    "    dataset_counter = 1\n",
    "    note_counter = 0\n",
    "\n",
    "    # Process the remaining rows\n",
    "    for index, row in remaining_df.iterrows():\n",
    "        if row[0] == \"Note\":\n",
    "            note_counter += 1\n",
    "            if note_counter > 1:\n",
    "                datasets[f\"dataset_{dataset_counter}\"] = pd.DataFrame(current_dataset)\n",
    "                dataset_counter += 1\n",
    "                current_dataset = []\n",
    "        current_dataset.append(row)\n",
    "\n",
    "    # Append the last dataset if exists\n",
    "    if current_dataset:\n",
    "        datasets[f\"dataset_{dataset_counter}\"] = pd.DataFrame(current_dataset)\n",
    "\n",
    "    # Create a dataframe for each dataset and show the names of the dataframes created\n",
    "    dataframe_names = []\n",
    "    for key, dataset in datasets.items():\n",
    "        globals()[key] = dataset\n",
    "        dataframe_names.append(key)\n",
    "\n",
    "    # Apply the operations to each dataset\n",
    "    for key, dataset in datasets.items():\n",
    "        # Convert the second column to datetime, coercing errors\n",
    "        dataset.iloc[3:, 1] = pd.to_datetime(dataset.iloc[3:, 1], format='%d/%m/%Y %H:%M:%S', errors='coerce')\n",
    "        \n",
    "        # Replace all commas with periods in columns 2 to the end\n",
    "        dataset.iloc[:, 1:] = dataset.iloc[:, 1:].replace(',', '.', regex=True)\n",
    "        \n",
    "        # Convert subsequent columns to float, coercing errors\n",
    "        for col in dataset.columns[2:]:\n",
    "            dataset.iloc[2:, dataset.columns.get_loc(col)] = pd.to_numeric(dataset.iloc[2:, dataset.columns.get_loc(col)], errors='coerce')\n",
    "        \n",
    "        # Update the dataset in the dictionary\n",
    "        datasets[key] = dataset\n",
    "\n",
    "    # Create the dataframe for storing general data\n",
    "    riassunto = pd.DataFrame(columns=index_row)\n",
    "    new_columns = []\n",
    "    riassunto = pd.concat([riassunto, pd.DataFrame(columns=new_columns)], axis=1)\n",
    "\n",
    " # Calculate and assign time delta values for the second column of riassunto\n",
    "    for key, dataset in datasets.items():\n",
    "        # Ensure the values are Timestamps\n",
    "        start_time = pd.to_datetime(dataset.iloc[3, 1], errors='coerce')\n",
    "        end_time = pd.to_datetime(dataset.iloc[-1, 1], errors='coerce')\n",
    "        time_delta = end_time - start_time\n",
    "        row_index = dataframe_names.index(key)\n",
    "        riassunto.at[row_index, riassunto.columns[1]] = time_delta\n",
    "\n",
    "    # Assign values from datasets to riassunto sequentially in column 1\n",
    "    row_counter = 0\n",
    "    for key, dataset in datasets.items():\n",
    "        value_to_assign = dataset.iloc[1, 0]  # Value from column 1 row 2\n",
    "        riassunto.at[row_counter, 'Note'] = value_to_assign\n",
    "        row_counter += 1\n",
    "\n",
    "    # Iterate over each dataset\n",
    "    for key, dataset in datasets.items():\n",
    "        # Calculate the average values for each column starting from the third column\n",
    "        averages = dataset.iloc[2:, 2:].mean()\n",
    "        \n",
    "        # Assign the average values to the corresponding row in riassunto\n",
    "        row_index = dataframe_names.index(key)\n",
    "        for col_index, avg_value in enumerate(averages, start=2):\n",
    "            riassunto.iloc[row_index, col_index] = avg_value\n",
    "\n",
    "    # Create new columns for minimum and maximum values\n",
    "    avg_columns = []\n",
    "    min_columns = []\n",
    "    max_columns = []\n",
    "\n",
    "    for col in riassunto.columns[1:]:\n",
    "        avg_columns.append(f\"{col}_std\")\n",
    "        min_columns.append(f\"{col}_min\")\n",
    "        max_columns.append(f\"{col}_max\")\n",
    "        \n",
    "\n",
    "    # Append new columns to riassunto dataframe\n",
    "    riassunto = pd.concat([riassunto, pd.DataFrame(columns=avg_columns + min_columns + max_columns)], axis=1)\n",
    "\n",
    "    # Iterate over each dataset\n",
    "    for key, dataset in datasets.items():\n",
    "        # Calculate the minimum and maximum values for each column starting from the third column\n",
    "        deviations = dataset.iloc[2:, 2:].std()\n",
    "        min_values = dataset.iloc[2:, 2:].min()\n",
    "        max_values = dataset.iloc[2:, 2:].max()\n",
    "        \n",
    "        # Assign the minimum values to the corresponding row in riassunto\n",
    "        row_index = dataframe_names.index(key)\n",
    "        for col_index, deviations in enumerate(deviations, start=2):\n",
    "            riassunto.iloc[row_index, len(riassunto.columns)//3 + col_index] = deviations\n",
    "        \n",
    "        for col_index, min_value in enumerate(min_values, start=2):\n",
    "            riassunto.iloc[row_index, len(riassunto.columns)//3 + col_index] = min_value\n",
    "        \n",
    "        # Assign the maximum values to the corresponding row in riassunto\n",
    "        for col_index, max_value in enumerate(max_values, start=2):\n",
    "            riassunto.iloc[row_index, 2*len(riassunto.columns)//3 + col_index] = max_value\n",
    "\n",
    "\n",
    "    # Assign 'File name' and 'Last modification' values from row 20 of each dataframe\n",
    "    riassunto['File name'] = df.at[20,'File name']\n",
    "    riassunto['Last modification'] = df.at[20,'Last modification']\n",
    "    #riassunto.at[20, 'File name'] = df.at[20, 'File name']\n",
    "    #riassunto.at[20, 'Last modification'] = df.at[20, 'Last modification']\n",
    "    updated_riassunto = append_columns_to_riassunto(riassunto, test_data_columns, test_data_values) \n",
    "\n",
    "\n",
    "    # Assign the riassunto dataframe to a variable name sequentially\n",
    "    globals()[f'riassunto_{i+1}'] = updated_riassunto\n",
    "    riassunto_dataframes.append(updated_riassunto)\n",
    "\n",
    "for i in range(1, len(globals())):\n",
    "    var_name = f'riassunto_{i}'\n",
    "    if var_name in globals():\n",
    "        globals()[var_name] = process_dataframe(globals()[var_name])\n",
    "\n",
    "# Print the processed riassunto dataframes for verification\n",
    "for i in range(1, len(globals())):\n",
    "    var_name = f'riassunto_{i}'\n",
    "\n",
    "header = riassunto_1.columns.tolist()\n",
    "# Loop through each DataFrame and replace column names with values from 'header'\n",
    "for i, df in enumerate(riassunto_dataframes):\n",
    "    df.columns = header\n",
    "\n",
    "for i in range(len(riassunto_dataframes)):\n",
    "    riassunto_dataframes[i]=riassunto_dataframes[i].loc[~riassunto_dataframes[i].index.to_series().duplicated(keep='first')]\n",
    "\n",
    "final_dataframe = pd.concat(riassunto_dataframes, ignore_index=True)\n",
    "\n",
    "# Sort the dataframe by 'DATA/ORA COLLAUDO'\n",
    "final_dataframe.sort_values(by='Last modification', inplace=True)\n",
    "\n",
    "# Create a new column 'numero.test' with sequential values based on the sorted dates\n",
    "final_dataframe['numero.test'] = final_dataframe.groupby('File name').ngroup().apply(lambda x: f'P{x+1:06d}')\n",
    "\n",
    "# Ensure that rows with the same 'File Name' have the same 'numero.test'\n",
    "final_dataframe['numero.test'] = final_dataframe.groupby('File name')['numero.test'].transform('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'00312386_TALA8NMSBA10000R03_20230206_105757.txt': 1, '00312386_TALA8NMSBA10000R03_20230206_115156.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230213_070657.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230213_113726.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230214_085849.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230215_070409.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230216_070625.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230216_110150.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230220_123518.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230224_105251.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230224_113632.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230227_080206.txt': 1, '00312855_PROTOTIPO_TALA3_GROBR00_20230228_080215.txt': 1, '00314070_TAO93NMIBXGT100R01_20230223_145203.txt': 1, '00314070_TAO93NMIBXGT100R01_20230224_070230 - Copia.txt': 1, '00314070_TAO93NMIBXGT100R01_20230224_070230.txt': 1, '00314070_TAO93NMIBXGT100R01_20230225_070746.txt': 1, '00314804_TAL37NBSBXED200R05_20230213_140859.txt': 1, '00314804_TAL37NBSBXED200R05_20230214_072859.txt': 1, '00314804_TAL37NBSBXED200R05_20230214_072859_1.txt': 1, '00314854_TAL93NNHBA10098R00_20230228_140527.txt': 1, '00314854_TAL93NNHBA10098R00_20230301_080034.txt': 1, '00314854_TAL93NNHBA10098R00_20230302_112926.txt': 1, '00314854_TAL93NNHBA10098R00_20230303_082505.txt': 1, '00314856_TAOA3PLVBXIS000R04_20230208_151013.txt': 1, '00314856_TAOA3PLVBXIS000R04_20230209_070652.txt': 1, '00315015_TAL24NBSBA00000R04_20230217_114405 - Copia (2).txt': 1, '00315015_TAL24NBSBA00000R04_20230217_114405 - Copia.txt': 1, '00315015_TAL24NBSBA00000R04_20230217_114405.txt': 1, '00315015_TAL24NBSBA00000R04_20230220_123919 - Copia.txt': 1, '00315015_TAL24NBSBA00000R04_20230220_123919.txt': 1, '00315015_TAL24NBSBA00000R04_20230222_101538.txt': 1, '00315032_TCW19NBSBXED000R03_20230215_152137.txt': 1, '00315032_TCW19NBSBXED000R03_20230216_085343.txt': 1, '00315032_TCW19NBSBXED000R03_20230216_112958.txt': 1, '00315158_TAL37NHHBXTC000R00_20230203_092503.txt': 1, '00315158_TAL37NHHBXTC000R00_20230204_073843.txt': 1, '00315158_TAL37NHHBXTC000R00_20230207_130806.txt': 1, '00315158_TAL37NHHBXTC000R00_20230208_070202.txt': 1, '00315692_TALB9NHHBXET000R00_20230207_084905.txt': 1, '00316312_TALE6NMSBXUP800R03_20230228_080357.txt': 1, '00317467_CCWA9NHHBXOS011R00_20230929_103102_1.txt': 1, '00317778_TCW19NBSBA10023R01_20230204_071007.txt': 1, '00324111_SAW50RBSBXSC100R05_20230302_090333.txt': 1}\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "def count_files(code_list):\n",
    "    code_count = {}\n",
    "    for code in code_list:\n",
    "        if code in code_count:\n",
    "            code_count[code] += 1\n",
    "        else:\n",
    "            code_count[code] = 1\n",
    "    return code_count\n",
    "code_counts = count_files(skipped_files)\n",
    "print(code_counts)\n",
    "\n",
    "def count_items(item_list):\n",
    "    return len(item_list)\n",
    "item_count = count_items(skipped_files)\n",
    "\n",
    "skipped = pd.DataFrame(code_counts, columns=[\"Code\"])\n",
    "\n",
    "# Open a file dialog to select the directory to save the CSV file\n",
    "file_path2 = filedialog.asksaveasfilename(defaultextension=\".csv\", filetypes=[(\"CSV files\", \"*.csv\")])\n",
    "\n",
    "# Check if a file path was selected\n",
    "if file_path2:\n",
    "    # Export the dataframe as a CSV file\n",
    "    skipped.to_csv(\"output.csv\", index=False)\n",
    "    print(f\"The list has been successfully transformed into a dataframe and exported as {file_path2}.\")\n",
    "else:\n",
    "    print(\"No file path was selected.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
