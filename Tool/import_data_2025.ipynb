{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Function to fix some bugs\n",
    "def process_dataframe(df):\n",
    "    # Check if values from first column to column 1289 in the last row are NaN\n",
    "    if df.iloc[-1, :1289].isna().all():\n",
    "        # Assign the values contained in columns 1290 and 1291 of the last row to variables\n",
    "        col_1290_value = df.iloc[-1, 1289]\n",
    "        col_1291_value = df.iloc[-1, 1290]\n",
    "        \n",
    "        # Delete the last row\n",
    "        df = df.iloc[:-1, :]\n",
    "        \n",
    "        # Assign the values to the new last row\n",
    "        df.iloc[-1, 1289] = col_1290_value\n",
    "        df.iloc[-1, 1290] = col_1291_value\n",
    "    \n",
    "    return df\n",
    "\n",
    "def append_columns_to_riassunto(riassunto, columns, values):\n",
    "    for col, val in zip(columns, values):\n",
    "        riassunto[col] = val\n",
    "    return riassunto\n",
    "\n",
    "def count_files(code_list):\n",
    "    code_count = {}\n",
    "    for code in code_list:\n",
    "        if code in code_count:\n",
    "            code_count[code] += 1\n",
    "        else:\n",
    "            code_count[code] = 1\n",
    "    return code_count\n",
    "\n",
    "def count_items(item_list):\n",
    "    return len(item_list)\n",
    "\n",
    "# Function to convert column to datetime format from row 20 to the last\n",
    "def convert_column_to_datetime(df, column_index, start_row, date_format):\n",
    "    for i in range(start_row, len(df)):\n",
    "        try:\n",
    "            df.iloc[i, column_index] = pd.to_datetime(df.iloc[i, column_index], format=date_format)\n",
    "        except ValueError:\n",
    "            # Skip rows that cannot be converted\n",
    "            continue\n",
    "\n",
    "# Loop through the list of dataframes and apply the function\n",
    "def apply_conversion_to_dataframes(dataframes, column_index, start_row, date_format):\n",
    "    for i, df in enumerate(dataframes):\n",
    "        convert_column_to_datetime(df, column_index, start_row, date_format)\n",
    "#_______________________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "# Directory containing the txt files\n",
    "directory = r'V:\\TEST CHILLER\\TXT'\n",
    "\n",
    "# List to store dataframes and skipped files\n",
    "dataframes = []\n",
    "skipped_files = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for i, filename in enumerate(os.listdir(directory)):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            # First method to read the file\n",
    "            df = pd.read_csv(file_path, delimiter=\"\\t\", header=None)\n",
    "            df = df.drop(17).reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                # Second method to read the file if the first fails\n",
    "                total_columns = 321\n",
    "                df1 = pd.read_csv(file_path, header=None, engine='python', encoding='latin', sep='\\t', nrows=17)\n",
    "                df2 = pd.read_csv(file_path, header=None, engine='python', encoding='latin', sep='\\t', skiprows=18)\n",
    "                df = pd.concat([df1, df2], axis=0).reset_index(drop=True)\n",
    "            except Exception as e:\n",
    "                # If both methods fail, skip the file and add to skipped_files list\n",
    "                skipped_files.append(filename)\n",
    "                continue\n",
    "        \n",
    "        # Add 'File name' and 'Last modification' columns at row 20\n",
    "        df.loc[20, 'File name'] = filename\n",
    "        df.loc[20, 'Last modification'] = datetime.fromtimestamp(os.path.getmtime(file_path)).strftime('%d/%m/%Y %H:%M:%S')\n",
    "        \n",
    "        # Append the dataframe to the list\n",
    "        dataframes.append(df)\n",
    "        \n",
    "        # Assign the dataframe to a variable name sequentially\n",
    "        globals()[f'df_{i+1}'] = df\n",
    "\n",
    "apply_conversion_to_dataframes(dataframes, 1, 20, '%d/%m/%Y %H:%M:%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to check if all DataFrames have the same number of columns\n",
    "def check_columns_consistency(dataframes):\n",
    "    if not dataframes:\n",
    "        return True  # If the list is empty, return True\n",
    "    \n",
    "    # Get the number of columns in the first DataFrame\n",
    "    num_columns = dataframes[0].shape[1]\n",
    "    \n",
    "    # Check if all DataFrames have the same number of columns\n",
    "    for df in dataframes:\n",
    "        if df.shape[1] != num_columns:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "consistent_columns = check_columns_consistency(dataframes)\n",
    "consistent_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a DataFrame with the name of the DataFrame and the number of columns\n",
    "def create_summary_dataframe(dataframes):\n",
    "    summary_data = []\n",
    "    for i, df in enumerate(dataframes):\n",
    "        df_name = f\"df{i+1}\"\n",
    "        summary_data.append({\"DataFrame Name\": df_name, \"Number of Columns\": df.shape[1]})\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return summary_df\n",
    "\n",
    "# Create the summary DataFrame\n",
    "summary_df = create_summary_dataframe(dataframes)\n",
    "# Export the DataFrame to a CSV file\n",
    "summary_df.to_csv(\"summary_output.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames with 323 columns: ['df1', 'df2', 'df3', 'df4', 'df5', 'df6', 'df7', 'df8', 'df9', 'df10', 'df11', 'df12', 'df13', 'df14', 'df15', 'df16', 'df17', 'df18']\n",
      "DataFrames with 318 columns: []\n"
     ]
    }
   ],
   "source": [
    "# Function to split the list of DataFrames into two sublists based on the number of columns\n",
    "import pandas as pd\n",
    "\n",
    "def split_dataframes(dataframes):\n",
    "    dataframes_323 = []\n",
    "    dataframes_318 = []\n",
    "    dataframes_323_names = []\n",
    "    dataframes_318_names = []\n",
    "\n",
    "    for df in dataframes:\n",
    "        if df.shape[1] == 323:\n",
    "            dataframes_323.append(df)\n",
    "            dataframes_323_names.append(df.name)\n",
    "        elif df.shape[1] == 318:\n",
    "            dataframes_318.append(df)\n",
    "            dataframes_318_names.append(df.name)\n",
    "\n",
    "    return dataframes_323, dataframes_318, dataframes_323_names, dataframes_318_names\n",
    "\n",
    "# Assign names to the sample dataframes\n",
    "for i, df in enumerate(dataframes):\n",
    "    df.name = f'df{i+1}'\n",
    "\n",
    "# Split the list of DataFrames\n",
    "dataframes_323, dataframes_318, dataframes_323_names, dataframes_318_names = split_dataframes(dataframes)\n",
    "\n",
    "# Display the lists of DataFrame names\n",
    "print(\"DataFrames with 323 columns:\", dataframes_323_names)\n",
    "print(\"DataFrames with 318 columns:\", dataframes_318_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = dataframes_323[0].iloc[17]\n",
    "\n",
    "# Iterate over all dataframes in dataframes_323\n",
    "for df in dataframes_323:\n",
    "    # Replace the 17th row with the baseline values\n",
    "    df.iloc[17] = baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the exceptions list\n",
    "exceptions_323 = []\n",
    "\n",
    "# Iterate over the dataframes starting from the second one\n",
    "for df in dataframes_323[1:]:\n",
    "    # Compare the 17th row with the baseline\n",
    "    if not df.iloc[17].equals(baseline):\n",
    "        # If there is a difference, add the dataframe to exceptions list\n",
    "        exceptions_323.append(df)\n",
    "\n",
    "# Remove the exceptions from the original list\n",
    "dataframes_323 = [df for df in dataframes_323 if not any(df.equals(exc) for exc in exceptions_323)]\n",
    "\n",
    "# Print the results\n",
    "print(len(dataframes_323))\n",
    "print(len(exceptions_323))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Dataframe Index, Column ID, Baseline Value, Found Value]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store differences\n",
    "differences = []\n",
    "\n",
    "# Iterate over the dataframes in exceptions_323\n",
    "for i, df in enumerate(exceptions_323):\n",
    "    # Compare the 17th row with the baseline\n",
    "    diff = df.iloc[17] != baseline\n",
    "    if diff.any():\n",
    "        # Store the index of the dataframe, the differing columns, baseline value, and found value\n",
    "        for col in diff.index[diff]:\n",
    "            differences.append((i, col, baseline[col], df.iloc[17][col]))\n",
    "\n",
    "# Create a dataframe to store the differences\n",
    "diff_df = pd.DataFrame(differences, columns=['Dataframe Index', 'Column ID', 'Baseline Value', 'Found Value'])\n",
    "\n",
    "# Print the differences dataframe\n",
    "print(diff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to drop\n",
    "columns_to_drop = list(range(2, 18)) + list(range(24, 114))  + list(range(122, 128)) + list(range(132, 144)) + list(range(148, 219)) + \\\n",
    "                  list(range(220, 223)) + list(range(224, 227)) + list(range(231, 234)) + list(range(235, 238)) + \\\n",
    "                  list(range(242, 245)) + list(range(246, 249)) + list(range(253, 256)) + list(range(257, 260)) + \\\n",
    "                  list(range(264, 267)) + list(range(268, 271)) + [281, 288, 295] + list(range(311, 321))\n",
    "\n",
    "# Iterate over all dataframes in dataframes_323 and drop the specified columns\n",
    "for df in dataframes_323:\n",
    "    df.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all dataframes in dataframes_323 and reset the column indexes\n",
    "for df in dataframes_323:\n",
    "    df.columns = range(df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_df2 = pd.DataFrame({\n",
    "    \"Index\": range(len(dataframes_323[0].columns)),\n",
    "    \"Column Names\": dataframes_323[0].iloc[17]\n",
    "})\n",
    "\n",
    "# Export this dataframe as a CSV file\n",
    "columns_df2.to_csv(\"columns_of_dataframe323.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to move\n",
    "columns_to_move = [25, 30, 45, 40, 45]\n",
    "\n",
    "# Iterate over all dataframes in dataframes_323\n",
    "for i, df in enumerate(dataframes_323):\n",
    "    # Move the specified columns to the end\n",
    "    cols = df.columns.tolist()\n",
    "    for col in columns_to_move:\n",
    "        cols.append(cols.pop(cols.index(col)))\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Reset the column indexes\n",
    "    df.columns = range(df.shape[1])\n",
    "    \n",
    "    # Update the dataframe in the list\n",
    "    dataframes_323[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names  = dataframes_323[0].iloc[17].tolist()\n",
    "column_names[79] = 'File name'\n",
    "column_names[80] = 'Last modification'\n",
    "header = pd.DataFrame(columns=column_names)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the lists\n",
    "#avg_columns = []\n",
    "#min_columns = []\n",
    "#max_columns = []\n",
    "\n",
    "#for col in header.columns[1:-2]:\n",
    "#    avg_columns.append(f\"{col}_std\")\n",
    "#    min_columns.append(f\"{col}_min\")\n",
    "#    max_columns.append(f\"{col}_max\")\n",
    "\n",
    "# Add the new columns to the dataframe HEADER\n",
    "#for col in avg_columns + min_columns + max_columns:\n",
    "#    header[col] = None\n",
    "\n",
    "#header.to_csv(\"header.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all dataframes should have the same columns as the first dataframe\n",
    "expected_columns = dataframes_323[0].columns\n",
    "\n",
    "# Lists to hold dataframes with matching and different columns\n",
    "matching_dataframes = []\n",
    "different_dataframes = []\n",
    "\n",
    "# Separate dataframes based on column names\n",
    "for df in dataframes_323:\n",
    "    if df.columns.equals(expected_columns):\n",
    "        matching_dataframes.append(df)\n",
    "    else:\n",
    "        different_dataframes.append(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dataframes have a string entry at row 20 in the 'File name' column: False\n"
     ]
    }
   ],
   "source": [
    "# Check if the entry at row 20 in the 'File name' column is of string type\n",
    "all_strings = True\n",
    "for df in dataframes_323:\n",
    "    try:\n",
    "        entry = df.at[20, 'File name']\n",
    "        if not isinstance(entry, str):\n",
    "            all_strings = False\n",
    "            break\n",
    "    except KeyError:\n",
    "        all_strings = False\n",
    "        break\n",
    "\n",
    "print(f\"All dataframes have a string entry at row 20 in the 'File name' column: {all_strings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "riassunto_dataframes = []\n",
    "column_names  = dataframes_323[0].iloc[17].tolist()\n",
    "baseline_df = dataframes_323[0].iloc[:17]\n",
    "baseline_df = baseline_df.loc[:, baseline_df.notna().any(axis=0)]\n",
    "baseline_df = baseline_df.drop(14)\n",
    "baseline_first_column = baseline_df.iloc[:, 0].tolist()\n",
    "\n",
    "# Process each dataframe and create riassunto dataframes\n",
    "for i, df in enumerate(dataframes_323):\n",
    "    # Split the dataframe\n",
    "    test_id = df.iloc[:17]\n",
    "    test_id = test_id.loc[:, test_id.notna().any(axis=0)]\n",
    "    index_row = df.iloc[17]\n",
    "    remaining_df = df.iloc[17:]\n",
    "    test_id = test_id.drop(14)\n",
    "    aligned_test_id = pd.DataFrame(columns=test_id.columns)\n",
    "\n",
    "    for value in baseline_first_column:\n",
    "            if value in test_id.iloc[:, 0].values:\n",
    "                row_index = test_id[test_id.iloc[:, 0] == value].index[0]\n",
    "                aligned_test_id = pd.concat([aligned_test_id, test_id.loc[[row_index]]])\n",
    "            else:\n",
    "                new_row = pd.Series([value, \"Value not declared\"], index=test_id.columns)\n",
    "                aligned_test_id = pd.concat([aligned_test_id, new_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "    # Append the remaining rows\n",
    "    aligned_test_id.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Initialize variables\n",
    "    datasets = {}\n",
    "    current_dataset = []\n",
    "    dataset_counter = 1\n",
    "    note_counter = 0\n",
    "\n",
    "    # Process the remaining rows\n",
    "\n",
    "    for index, row in remaining_df.iterrows(): \n",
    "            if row[0] == \"Note\":\n",
    "                note_counter += 1\n",
    "                if note_counter > 1:\n",
    "                    datasets[f\"dataset_{dataset_counter}\"] = pd.DataFrame(current_dataset)\n",
    "                    dataset_counter += 1\n",
    "                    current_dataset = []\n",
    "            current_dataset.append(row)\n",
    " \n",
    "\n",
    "    # Append the last dataset if exists\n",
    "    if current_dataset:\n",
    "        datasets[f\"dataset_{dataset_counter}\"] = pd.DataFrame(current_dataset)\n",
    "\n",
    "    # Create a dataframe for each dataset and show the names of the dataframes created\n",
    "\n",
    "    dataframe_names = []\n",
    "\n",
    "    for key, dataset in datasets.items():\n",
    "        globals()[key] = dataset\n",
    "        dataframe_names.append(key)\n",
    "\n",
    "    # Apply the operations to each dataset\n",
    "    for key, dataset in datasets.items():\n",
    "\n",
    "        # Replace all commas with periods in columns 2 to the end\n",
    "        dataset.iloc[:, 1:] = dataset.iloc[:, 1:].replace(',', '.', regex=True)\n",
    "        \n",
    "        # Convert subsequent columns to float, coercing errors\n",
    "        for col in dataset.columns[2:-2]:\n",
    "            dataset[col] = pd.to_numeric(dataset[col], errors='coerce')\n",
    "            dataset.iloc[2:, dataset.columns.get_loc(col)] = pd.to_numeric(dataset.iloc[2:, dataset.columns.get_loc(col)], errors='coerce')\n",
    "        \n",
    "        # Update the dataset in the dictionary\n",
    "        datasets[key] = dataset\n",
    "\n",
    "    # Create the dataframe for storing general data\n",
    "    riassunto = pd.DataFrame(columns=column_names)\n",
    "    new_columns = []\n",
    "    riassunto = pd.concat([riassunto, pd.DataFrame(columns=new_columns)], axis=1)\n",
    "\n",
    "    for key, dataset in datasets.items():\n",
    "        if len(dataset) < 4:\n",
    "            time_delta = \"Cannot be calculated\"\n",
    "        else:\n",
    "            # Ensure the values are Timestamps\n",
    "            start_row_index = 3\n",
    "            start_time = dataset.iloc[start_row_index, 1]\n",
    "            \n",
    "            # Check if start_time is valid\n",
    "            while pd.isna(start_time) or isinstance(start_time, str):\n",
    "                start_row_index += 1\n",
    "                if start_row_index >= len(dataset):  # Ensure we don't go out of bounds\n",
    "                    start_time = None\n",
    "                    break\n",
    "                start_time = dataset.iloc[start_row_index, 1]\n",
    "            \n",
    "            end_row_index = len(dataset) - 1\n",
    "            end_time = dataset.iloc[end_row_index, 1]\n",
    "            \n",
    "            # Check if end_time is valid\n",
    "            while pd.isna(end_time) or isinstance(end_time, str):\n",
    "                end_row_index -= 1\n",
    "                if end_row_index < start_row_index:  # Ensure we don't go out of bounds\n",
    "                    end_time = None\n",
    "                    break\n",
    "                end_time = dataset.iloc[end_row_index, 1]\n",
    "            \n",
    "            if start_time is None or end_time is None:\n",
    "                time_delta = \"Cannot be calculated\"\n",
    "            else:\n",
    "                time_delta = end_time - start_time\n",
    "        \n",
    "        row_index = dataframe_names.index(key)\n",
    "        riassunto.at[row_index, riassunto.columns[1]] = time_delta\n",
    "\n",
    "    # Assign values from datasets to riassunto sequentially in column 1\n",
    "    row_counter = 0\n",
    "    for key, dataset in datasets.items():\n",
    "        if len(dataset) < 2:\n",
    "            value_to_assign = \"Not enough data\"\n",
    "            riassunto.at[row_index, 'Note'] = value_to_assign\n",
    "        else:\n",
    "            value_to_assign = dataset.iloc[1, 0]  # Value from column 1 row 2\n",
    "            row_index = dataframe_names.index(key)\n",
    "            riassunto.at[row_index, 'Note'] = value_to_assign\n",
    "            row_counter += 1\n",
    "\n",
    "    # Iterate over each dataset\n",
    "    for key, dataset in datasets.items():\n",
    "\n",
    "        # Calculate the average values for each column starting from the third column\n",
    "        averages = dataset.iloc[2:, 2:-2].mean()\n",
    "        \n",
    "        # Assign the average values to the corresponding row in riassunto\n",
    "        row_index = dataframe_names.index(key)\n",
    "        \n",
    "        for col_index, avg_value in enumerate(averages, start=2):\n",
    "            riassunto.iloc[row_index, col_index] = avg_value\n",
    "\n",
    "    # Create new columns for minimum and maximum values\n",
    "    #std_columns = []\n",
    "    #min_columns = []\n",
    "    #max_columns = []\n",
    "\n",
    "    #for col in riassunto.columns[2:-2]:\n",
    "        #std_columns.append(f\"{col}_std\")\n",
    "        #min_columns.append(f\"{col}_min\")\n",
    "        #max_columns.append(f\"{col}_max\")\n",
    "        \n",
    "\n",
    "    # Append new columns to riassunto dataframe\n",
    "    #riassunto = pd.concat([riassunto, pd.DataFrame(columns=std_columns + min_columns + max_columns)], axis=1)\n",
    "\n",
    "    # Iterate over each dataset\n",
    "    #for key, dataset in datasets.items():\n",
    "    #    # Calculate the minimum and maximum values for each column starting from the third column\n",
    "    #    deviations = dataset.iloc[2:, 2:-2].std()\n",
    "    #    min_values = dataset.iloc[2:, 2:-2].min()\n",
    "    #    max_values = dataset.iloc[2:, 2:-2].max()\n",
    "    #    \n",
    "    #    # Assign the minimum values to the corresponding row in riassunto\n",
    "    #    row_index = dataframe_names.index(key)\n",
    "    #    for col_index, deviations in enumerate(deviations, start=2):\n",
    "    #        riassunto.iloc[row_index, len(riassunto.columns)//3 + col_index] = deviations\n",
    "    #    \n",
    "    #    for col_index, min_value in enumerate(min_values, start=2):\n",
    "    #        riassunto.iloc[row_index, len(riassunto.columns)//3 + col_index] = min_value\n",
    "    #    \n",
    "    #    # Assign the maximum values to the corresponding row in riassunto\n",
    "    #    for col_index, max_value in enumerate(max_values, start=2):\n",
    "    #        riassunto.iloc[row_index, 2*len(riassunto.columns)//3 + col_index] = max_value\n",
    "\n",
    "# Assign the maximum values to the corresponding row in riassunto\n",
    "    # Check if 'File name' and 'Last modification' columns exist before assigning\n",
    "    #if 'File name' in df.columns and 'Last modification' in df.columns:\n",
    "        riassunto['File name'] = dataframes_323[i].iloc[20,79]\n",
    "        riassunto['Last modification'] = dataframes_323[i].iloc[20,80]\n",
    "    #else:\n",
    "    #    riassunto['File name'] = 'Data not found'\n",
    "    #    riassunto['Last modification'] = 'Data not found'\n",
    "    riassunto.reset_index(drop=True, inplace=True)\n",
    "    updated_riassunto = append_columns_to_riassunto(riassunto, aligned_test_id.transpose().iloc[0], aligned_test_id.transpose()[1:].iloc[0]) \n",
    "\n",
    "# Assin the riassunto dataframe to a variable name sequentially\n",
    "    globals()[f'riassunto_{i+1}'] = updated_riassunto\n",
    "    riassunto_dataframes.append(updated_riassunto)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dataframes have the same number of columns.\n"
     ]
    }
   ],
   "source": [
    "# Check which of the dataframes in the list 'riassunto_dataframes' does not have the same number of columns\n",
    "def find_inconsistent_dataframes(dataframes):\n",
    "    num_columns = [df.shape[1] for df in dataframes]\n",
    "    max_columns = max(num_columns)\n",
    "    inconsistent_dfs = [i for i, n in enumerate(num_columns) if n != max_columns]\n",
    "    return inconsistent_dfs\n",
    "\n",
    "# Find the inconsistent dataframes in riassunto_dataframes\n",
    "inconsistent_dataframes = find_inconsistent_dataframes(riassunto_dataframes)\n",
    "\n",
    "if inconsistent_dataframes:\n",
    "    print(f\"The following dataframes do not have the same number of columns: {inconsistent_dataframes}\")\n",
    "else:\n",
    "    print(\"All dataframes have the same number of columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Note</th>\n",
       "      <th>Duration</th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P3</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>...</th>\n",
       "      <th>AVAILABLE.TEST.ID</th>\n",
       "      <th>AVAILABLE.N.PROVA</th>\n",
       "      <th>EXTRACTED TEXT</th>\n",
       "      <th>HELPER22</th>\n",
       "      <th>HELPER</th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>helper3</th>\n",
       "      <th>duration2</th>\n",
       "      <th>SUPPORT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>circ.1 - precarica 5,0 kg VT tutta aperta</td>\n",
       "      <td>0 days 00:04:40</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.101754</td>\n",
       "      <td>4.24386</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>23.740877</td>\n",
       "      <td>23.840702</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CIRC.1 - 5,0 kg VT tutta aperta - report con p...</td>\n",
       "      <td>0 days 00:04:45</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.410345</td>\n",
       "      <td>3.731034</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>26.354655</td>\n",
       "      <td>26.370862</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CIRC.2 - 5,0 kg VT tutta aperta - report con p...</td>\n",
       "      <td>0 days 00:05:20</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.410769</td>\n",
       "      <td>3.735385</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>27.549385</td>\n",
       "      <td>27.564462</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CIRC.3 - 5,0 kg VT tutta aperta - report con p...</td>\n",
       "      <td>0 days 00:04:50</td>\n",
       "      <td>1.618644</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.40339</td>\n",
       "      <td>3.749153</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>28.725763</td>\n",
       "      <td>28.569153</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CIRC.4 - 5,0 kg VT tutta aperta - report con p...</td>\n",
       "      <td>0 days 00:04:35</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.439286</td>\n",
       "      <td>3.725</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.8</td>\n",
       "      <td>29.5225</td>\n",
       "      <td>29.367143</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Note         Duration  \\\n",
       "0          circ.1 - precarica 5,0 kg VT tutta aperta  0 days 00:04:40   \n",
       "1  CIRC.1 - 5,0 kg VT tutta aperta - report con p...  0 days 00:04:45   \n",
       "2  CIRC.2 - 5,0 kg VT tutta aperta - report con p...  0 days 00:05:20   \n",
       "3  CIRC.3 - 5,0 kg VT tutta aperta - report con p...  0 days 00:04:50   \n",
       "4  CIRC.4 - 5,0 kg VT tutta aperta - report con p...  0 days 00:04:35   \n",
       "\n",
       "         P1   P2        P3        P4   P5   P6         T1         T2  ...  \\\n",
       "0       1.6  1.6  0.101754   4.24386  1.7  1.6  23.740877  23.840702  ...   \n",
       "1       1.6  1.6  0.410345  3.731034  1.7  1.7  26.354655  26.370862  ...   \n",
       "2       1.6  1.6  0.410769  3.735385  1.7  1.7  27.549385  27.564462  ...   \n",
       "3  1.618644  1.6   0.40339  3.749153  1.7  1.8  28.725763  28.569153  ...   \n",
       "4       1.7  1.6  0.439286     3.725  1.7  1.8    29.5225  29.367143  ...   \n",
       "\n",
       "  AVAILABLE.TEST.ID AVAILABLE.N.PROVA EXTRACTED TEXT HELPER22 HELPER Column1  \\\n",
       "0              None              None           None     None   None    None   \n",
       "1              None              None           None     None   None    None   \n",
       "2              None              None           None     None   None    None   \n",
       "3              None              None           None     None   None    None   \n",
       "4              None              None           None     None   None    None   \n",
       "\n",
       "  Column2 helper3 duration2 SUPPORT  \n",
       "0    None    None      None    None  \n",
       "1    None    None      None    None  \n",
       "2    None    None      None    None  \n",
       "3    None    None      None    None  \n",
       "4    None    None      None    None  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the global dataframe with modified column names\n",
    "global_df = pd.DataFrame(columns=[col for col in riassunto_1.columns])\n",
    "\n",
    "# Add another column with the name \"N.TEST\"\n",
    "global_df[\"N.TEST2\"] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first dataframe of the list riassunto_dataframes as baseline\n",
    "baseline_columns = riassunto_dataframes[0].columns\n",
    "\n",
    "# Assign the value of its column names to all other dataframes in the list\n",
    "for i in range(1, len(riassunto_dataframes)):\n",
    "    riassunto_dataframes[i].columns = baseline_columns\n",
    "\n",
    "# Assign a sequential test_id for each dataframe in riassunto_dataframes\n",
    "test_ids = [f\"t{i+1:03d}\" for i in range(len(riassunto_dataframes))]\n",
    "\n",
    "# Concatenate the rows of all dataframes in a global_df\n",
    "global_df = pd.DataFrame(columns=baseline_columns)\n",
    "global_df[\"N.TEST2\"] = None\n",
    "\n",
    "for i, df in enumerate(riassunto_dataframes):\n",
    "    df[\"N.TEST2\"] = test_ids[i]\n",
    "    global_df = pd.concat([global_df, df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe with column names has been exported as 'columns_of_global_df.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with an index column and another column containing the names of the columns of global_df\n",
    "columns_df = pd.DataFrame({\n",
    "    \"Index\": range(len(global_df.columns)),\n",
    "    \"Column Names\": global_df.columns\n",
    "})\n",
    "\n",
    "# Export this dataframe as a CSV file\n",
    "columns_df.to_csv(\"columns_of_global_df.csv\", index=False)\n",
    "\n",
    "print(\"The dataframe with column names has been exported as 'columns_of_global_df.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop columns at positions 79, 80, and 102\n",
    "columns_to_drop = [79, 80, 102]\n",
    "global_df.drop(global_df.columns[columns_to_drop], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 2: Rename the column \"Time Stamp\" to \"Duration\"\n",
    "global_df.rename(columns={\"Time Stamp\": \"Duration\"}, inplace=True)\n",
    "\n",
    "# Step 3: Create new columns\n",
    "new_columns = [\"TEST.ID\", \"SUPPORT\", \"N.PROVA\", \"AVAILABLE.TEST.ID\", \"AVAILABLE.N.PROVA\", \n",
    "               \"EXTRACTED TEXT\", \"HELPER22\", \"HELPER\", \"Column1\", \"Column2\", \"helper3\", \"duration2\"]\n",
    "for col in new_columns:\n",
    "    global_df[col] = None\n",
    "\n",
    "# Step 4: Reorder the columns as specified\n",
    "ordered_columns = [\n",
    "    \"STAZIONE DI COLLAUDO\", \"TEST.ID\", \"SUPPORT\", \"N.TEST2\", \"N.PROVA\", \"AVAILABLE.TEST.ID\", \n",
    "    \"AVAILABLE.N.PROVA\", \"OPERATORE\", \"CODICE MACCHINA\", \"MATRICOLA MACCHINA\", \"NUMERO ODL\", \n",
    "    \"DATA/ORA COLLAUDO\", \"EXTRACTED TEXT\", \"DATA/ORA COLLAUDO\", \"FLUIDO\", \"GAS REFRIGERANTE\", \"REVISIONE\", \n",
    "    \"LINEA DEL FLUIDO\", \"HELPER22\", \"NR CIRCUITI\", \"RESPONSABILE LINEA CHILLER\\\\CLIMA\", \n",
    "    \"DIREZIONE TECNICA\", \"ALIMENTAZIONE\", \"ESITO COLLAUDO\", \"File name\", \"Last modification\", \n",
    "    \"Note\", \"HELPER\", \"Duration\", \"P1\", \"P2\", \"P3\", \"P4\", \"P5\", \"P6\", \"T1\", \"T2\", \"T3\", \"T4\", \n",
    "    \"T5\", \"T6\", \"T7\", \"T8\", \"TC7\", \"TC8\", \"TC9\", \"TC10\", \"P7\", \"P8\", \"P9\", \"P10\", \"PRELE_1\", \n",
    "    \"ITOT_1\", \"PACTT_1\", \"CFTOT_1\", \"FTOT_1\", \"ITOT_2\", \"PACTT_2\", \"CFTOT_2\", \"FTOT_2\", \"ITOT_3\", \n",
    "    \"PACTT_3\", \"CFTOT_3\", \"FTOT_3\", \"ITOT_4\", \"PACTT_4\", \"CFTOT_4\", \"FTOT_4\", \"ITOT_5\", \"PACTT_5\", \n",
    "    \"CFTOT_5\", \"FTOT_5\", \"EVAP1_POWER\", \"EVAP1_Cp\", \"EVAP1_Rho\", \"EVAP1_dT\", \"EVAP1_TIN_M\", \n",
    "    \"EVAP1_TOUT_M\", \"EVAP2_POWER\", \"EVAP2_Cp\", \"EVAP2_Rho\", \"EVAP2_dT\", \"EVAP2_TIN_M\", \"EVAP2_TOUT_M\", \n",
    "    \"EVAP3_POWER\", \"EVAP3_Cp\", \"EVAP3_Rho\", \"EVAP3_dT\", \"EVAP3_TIN_M1\", \"EVAP3_TOUT_M1\", \"EVAP1_DP_H2O\", \n",
    "    \"EVAP2_DP_H2O\", \"EVAP3_DP_H2O\", \"TC_P7\", \"TC_P8\", \"TC_P9\", \"TC_P10\", \"SOTT_1\", \"SOTT_2\", \"SURR_1\", \n",
    "    \"SURR_2\", \"TAC_M\", \"MP1_2\", \"MP3_4\", \"MP5_6\", \"VCM_1\", \"VCM_2\", \"VCM_3\", \"VCM_4\", \"VCM_5\", \n",
    "    \"Column1\", \"Column2\", \"helper3\", \"duration2\"\n",
    "]\n",
    "\n",
    "global_df = global_df[ordered_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\e2023898\\AppData\\Local\\Temp\\ipykernel_35368\\2562068591.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  global_df[\"OPERATORE\"].replace({\"1 RESTANI MATTEO\": \"RESTANI\", \"1 CARLINI MIRKO\": \"CARLINI\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Replace the values in the \"OPERATORE\" column\n",
    "global_df[\"OPERATORE\"].replace({\"1 RESTANI MATTEO\": \"RESTANI\", \"1 CARLINI MIRKO\": \"CARLINI\"}, inplace=True)\n",
    "\n",
    "# Update TEST.ID based on changes in NUMERO ODL\n",
    "global_df['TEST.ID'] = 'CHANGED'\n",
    "global_df.loc[global_df['NUMERO ODL'].shift() != global_df['NUMERO ODL'], 'TEST.ID'] = 'CURRENT'\n",
    "\n",
    "# Run through the dataframe and check the column \"NUMERO ODL\"\n",
    "for i in range(1, len(global_df)):\n",
    "    if global_df.at[i, \"NUMERO ODL\"] != global_df.at[i - 1, \"NUMERO ODL\"]:\n",
    "        global_df.at[i, \"TEST.ID\"] = global_df.at[i - 1, \"TEST.ID\"]\n",
    "\n",
    "\n",
    "# Look for the characters \"_PA.txt\" in the \"GAS REFRIGERANTE\" column and delete them if found\n",
    "global_df[\"GAS REFRIGERANTE\"] = global_df[\"GAS REFRIGERANTE\"].str.replace(\"_PA.txt\", \"\", regex=False)\n",
    "\n",
    "# Copy the first 10 left characters from \"DATA/ORA COLLAUDO\" to \"EXTRACTED TEXT\"\n",
    "global_df[\"Column1\"] = global_df[\"CODICE MACCHINA\"].str[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been exported to 'global_df_comma.csv' with comma as the decimal separator.\n"
     ]
    }
   ],
   "source": [
    "# Loop through the DataFrame rows and round float values to two decimal places\n",
    "for index, row in global_df.iterrows():\n",
    "    for col in global_df.columns:\n",
    "        if isinstance(row[col], float):\n",
    "            global_df.at[index, col] = round(row[col], 2)\n",
    "\n",
    "# Function to convert decimal separator from . to , for display purposes\n",
    "def convert_decimal_separator(df):\n",
    "    df_str = df.copy()\n",
    "    for col in df_str.select_dtypes(include=['float']):\n",
    "        df_str[col] = df_str[col].map(lambda x: f\"{x:.2f}\".replace('.', ','))\n",
    "    return df_str\n",
    "\n",
    "# Convert the decimal separator for display\n",
    "global_df_display = convert_decimal_separator(global_df)\n",
    "\n",
    "# Export the DataFrame to CSV with comma as the decimal separator\n",
    "global_df_display.to_csv('dicember3.csv', index=False, sep=';')\n",
    "\n",
    "print(\"DataFrame has been exported to 'global_df_comma.csv' with comma as the decimal separator.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
